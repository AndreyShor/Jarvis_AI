The full modified update with rollback can be written as follows:

Calculate the target and TD error:
δ
=
r
+
γ
 
max
⁡
a
′
Q
(
s
′
,
a
′
)
−
Q
(
s
,
a
)
δ=r+γ 
a 
′
 
max
​	
 Q(s 
′
 ,a 
′
 )−Q(s,a)
Set the penalty factor:
β
=
{
1.25
,
if 
r
+
γ
 
max
⁡
a
′
Q
(
s
′
,
a
′
)
≤
0.5
 
Q
(
s
,
a
)
1
,
otherwise.
 
β={ 
1.25,
1,
​	
  
if r+γmax 
a 
′
 
​	
 Q(s 
′
 ,a 
′
 )≤0.5Q(s,a)
otherwise.
​	
 
Update the Q-value:
Q
(
s
,
a
)
←
Q
(
s
,
a
)
+
α
 
β
 
δ
.
Q(s,a)←Q(s,a)+αβδ.
Determine the next state:
s
next
=
{
s
,
if 
r
+
γ
 
max
⁡
a
′
Q
(
s
′
,
a
′
)
≤
0.5
 
Q
(
s
,
a
)
s
′
,
otherwise.
 
s 
next
​	
 ={ 
s,
s 
′
 ,
​	
  
if r+γmax 
a 
′
 
​	
 Q(s 
′
 ,a 
′
 )≤0.5Q(s,a)
otherwise.
​	
 In your modified DQN framework, the loss function becomes:

L
(
θ
)
=
E
[
(
β
(
r
+
γ
 
max
⁡
a
′
Q
(
s
next
,
a
′
;
θ
−
)
−
Q
(
s
,
a
;
θ
)
)
)
2
]
,
L(θ)=E[(β(r+γ 
a 
′
 
max
​	
 Q(s 
next
​	
 ,a 
′
 ;θ 
−
 )−Q(s,a;θ))) 
2
 ],
​	
 
with

β
=
{
1.25
,
if 
r
+
γ
 
max
⁡
a
′
Q
(
s
′
,
a
′
;
θ
−
)
≤
0.5
 
Q
(
s
,
a
;
θ
)
1
,
otherwise,
 
β={ 
1.25,
1,
​	
  
if r+γmax 
a 
′
 
​	
 Q(s 
′
 ,a 
′
 ;θ 
−
 )≤0.5Q(s,a;θ)
otherwise,
​	
 
and

s
next
=
{
s
,
if 
r
+
γ
 
max
⁡
a
′
Q
(
s
′
,
a
′
;
θ
−
)
≤
0.5
 
Q
(
s
,
a
;
θ
)
s
′
,
otherwise.
 
s 
next
​	
 ={ 
s,
s 
′
 ,
​	
  
if r+γmax 
a 
′
 
​	
 Q(s 
′
 ,a 
′
 ;θ 
−
 )≤0.5Q(s,a;θ)
otherwise.
​	
 
This modification means that when the network’s prediction is much higher than the observed return (i.e., when 
r
+
γ
 
max
⁡
a
′
Q
(
s
′
,
a
′
;
θ
−
)
r+γmax 
a 
′
 
​	
 Q(s 
′
 ,a 
′
 ;θ 
−
 ) is less than half of 
Q
(
s
,
a
;
θ
)
Q(s,a;θ)), you both:

Increase the penalty on the loss (scaling the TD error by 
β
=
1.25
β=1.25 instead of 1), and
“Roll back” the state by using 
s
s instead of 
s
′
s 
′
  in computing the target.
This design would be implemented in the loss computation stage (and also possibly affect how transitions are stored or sampled) so that during training, the network learns to correct for these “heavy mistake” situations.